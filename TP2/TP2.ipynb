{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "# Notebook Principal\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "- 101055 - Bojman, Camila\n",
    "- 100029 - del Mazo, Federico\n",
    "- 100687 - Hortas, Cecilia\n",
    "- 97649 - Souto, Rodrigo\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**\n",
    "\n",
    "**https://www.kaggle.com/datatouille2018/competitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando la investigación sobre la empresa Trocafone realizada en el [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html), se busca determinar la probabilidad de que un usuario del sitio realice una conversión en el período determinado.\n",
    "\n",
    "Notebooks en orden de corrida y lectura:\n",
    "\n",
    "0. [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html) --> Familiarización con el set de datos y exploración de estos.\n",
    "\n",
    "1. [Investigación Previa](https://fdelmazo.github.io/7506-Datos/TP2/investigacion.html) --> Con ayuda de lo trabajado en el TP1, se averiguan más cosas de las datos, en busqueda de que poder reutilizar.\n",
    "\n",
    "2. [Creación de Dataframes](https://fdelmazo.github.io/7506-Datos/TP2/new-dataframes.html) --> Como parte del feature engineering, se crean dataframes nuevos con información de los productos del sitio y de como se accede a este (marcas, sistemas operativos, etc).\n",
    "\n",
    "3. [Feature Engineering](https://fdelmazo.github.io/7506-Datos/TP2/feature-engineering.html) --> Busqueda de atributos de los usuarios de los cuales se busca predecir la conversión.\n",
    "\n",
    "4. [Submission Framework](https://fdelmazo.github.io/7506-Datos/TP2/submission-framework.html) --> Pequeño framework para construir las postulaciones de labels. \n",
    "\n",
    "5. TP2 (este notebook)--> Teniendo todo en cuenta, usando los dataframes con todos los atributos buscados y encontrados, se definen y aplican los algoritmos de clasificación, se realizan los entrenamientos y posteriores predicciones de conversiones y finalmente se arman las postulaciones de labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up inicial, se deja comentado para evitar instalarle módulos al usuario\n",
    "## Primero, descargar los datasets de no tenerlos\n",
    "\n",
    "# Antes de comenzar, setear las credenciales (usuario y token)\n",
    "\n",
    "# 1. Visitar: https://www.kaggle.com/datatouille2018/account (con la cuenta que sea)\n",
    "# 2. Tocar en Create New API Token\n",
    "# 3. Guardar el archivo descargado en ~/.kaggle/kaggle.json\n",
    "\n",
    "# !pip install kaggle # https://github.com/Kaggle/kaggle-api\n",
    "# !kaggle competitions download -c trocafone -p data\n",
    "# !unzip -q data/events_up_to_01062018.csv.zip -d data\n",
    "# !rm data/events_up_to_01062018.csv.zip\n",
    "# !ls data/\n",
    "\n",
    "## Luego, descargar los módulos a utilizar a lo largo de todo el trabajo\n",
    "\n",
    "# !pip install nbimporter\n",
    "# !conda install -c conda-forge xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter # pip install nbimporter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import submission_framework as SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False).set_index('person')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv').groupby('person').sum()\n",
    "\n",
    "display(df_users.head(), df_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.34\n",
    "\n",
    "def full_framework_wrapper(model_name, model_function, columns=df_users.columns.tolist(), test_size=test_size, seed=seed, verbosity=False, all_in=False, submit=False):\n",
    "    model_df_x = df_users[columns]\n",
    "    model_df_y = df_y\n",
    "    \n",
    "    X, y = SF.fr1_extract_X_y(model_df_x, model_df_y)\n",
    "    X_train, X_test, y_train, y_test = SF.fr2_train_test_split(X, y, seed, test_size)\n",
    "    if all_in:\n",
    "        model = model_function(X, y, seed)\n",
    "    else:\n",
    "        model = model_function(X_train, y_train, seed)\n",
    "    accuracy = SF.fr4_accuracy_score(X_test, y_test, model, model_name)\n",
    "    auc = SF.fr5_auc(X_test, y_test, model, model_name)\n",
    "    X_to_predict, predictions = SF.fr6_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "    \n",
    "    if verbosity: \n",
    "        feature_importances = SF.fr7_print_information(model_df_x, model, X_to_predict, True)\n",
    "        display(feature_importances)\n",
    "    \n",
    "    display('Model: {} - Accuracy: {:.4f} - AUC: {:.4f}'.format(model_name, accuracy, auc))  \n",
    "    \n",
    "    if submit:\n",
    "        csv_name, submission = SF.fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "        display(csv_name)\n",
    "        return auc, csv_name\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "def decision_tree(X_train, y_train, seed):\n",
    "    tree = DecisionTreeClassifier(criterion='gini',\n",
    "                              min_samples_leaf=5,\n",
    "                              min_samples_split=5,\n",
    "                              max_depth=3,\n",
    "                              random_state=seed)\n",
    "\n",
    "    tree.fit(X_train, y_train)\n",
    "    return tree\n",
    "\n",
    "full_framework_wrapper('decision_tree',decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_cols_1 = ['total_checkouts_month_5',\n",
    "'timestamp_last_checkout',\n",
    "'timestamp_last_event',\n",
    "'has_checkout_month_5',\n",
    "'total_checkouts',\n",
    "'days_to_last_event',\n",
    "'total_checkouts_last_week',\n",
    "'total_checkouts_months_1_to_4',\n",
    "'total_conversions',\n",
    "'total_session_conversions',\n",
    "'total_events',\n",
    "'total_sessions',\n",
    "'avg_events_per_session',\n",
    "'total_session_checkouts',\n",
    "'has_checkout']\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest(X_train, y_train, seed):\n",
    "    rf = RandomForestClassifier(n_estimators=20,\n",
    "                           n_jobs=2,\n",
    "                           min_samples_split=300,\n",
    "                           random_state=seed,\n",
    "                           class_weight='balanced')\n",
    "\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "\n",
    "full_framework_wrapper('random_forest',random_forest,columns=ss_cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DAR BOLA POR AHORA\n",
    "\n",
    "# No era necesario hacer el entrenamiento final con el modelo\n",
    "# entrenado con X_train. Conviene usar el set de entrenamiento\n",
    "# entero.\n",
    "\n",
    "# Lo de X_train y X_test es sólo (POR AHORA) para mostrar\n",
    "# el accuracy_score choto que estamos teniendo\n",
    "\n",
    "full_framework_wrapper('random_forest_allin',random_forest,all_in=True,submit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_name = 'knn'\n",
    "\n",
    "rango = 5\n",
    "def knn_gridsearch(x_train, y_train, x_test, y_test):\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    k_max = (0,0)\n",
    "    for k in range(1,100, rango):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "        knn.fit(x_train, y_train)\n",
    "        score = knn.score(x_test, y_test)*100\n",
    "        if score > k_max[1]:\n",
    "            k_max = (k,score)\n",
    "        print(\"K: {}, {}\".format(k, score))\n",
    "    a = k_max[0]\n",
    "    k_max = (0,0)\n",
    "    for k in range(a - rango, a + rango):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "        knn.fit(x_train, y_train)\n",
    "        score = knn.score(x_test, y_test)*100\n",
    "        if score > k_max[1]:\n",
    "            k_max = (k,score)\n",
    "        print(\"K: {}, {}\".format(k, score))\n",
    "    return k_max[0]\n",
    "\n",
    "K = knn_gridsearch(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def knn(X_train, y_train, seed, k=K):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "full_framework_wrapper(f'KNN{K}',knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb #conda install -c conda-forge xgboost \n",
    "\n",
    "def xgboost(X_train, y_train, seed):\n",
    "    xg_reg = xgb.XGBClassifier(objective='reg:linear',\n",
    "                class_weight='balanced',\n",
    "                colsample_bytree=0.3, learning_rate=0.1,\n",
    "                min_child_weight=2,\n",
    "                max_depth=5, alpha=10, n_estimators=20,\n",
    "                random_state=seed)\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    return xg_reg\n",
    "\n",
    "full_framework_wrapper('xgboost',xgboost,columns=ss_cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO BORRAR ESTO !!!!\n",
    "\n",
    "total_predictions = 0\n",
    "tmp_seed = seed\n",
    "for i in range(50):\n",
    "    tmp_seed = tmp_seed + i\n",
    "    model = xgboost(X, y, tmp_seed)\n",
    "    print(tmp_seed)\n",
    "    X_to_predict, predictions = SF.fr6_extract_X_to_predict(fr_df, fr_df_y, model)\n",
    "    total_predictions = total_predictions + predictions\n",
    "\n",
    "predictions = total_predictions / 50\n",
    "\n",
    "\"\"\"\n",
    "model = xgboost(X, y, seed)\n",
    "X_to_predict, predictions = SF.fr6_extract_X_to_predict(fr_df, fr_df_y, model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    if predictions[i] < 0:\n",
    "        print(\"WARNING: prediction[{}] = {}\".format(i, predictions[i]))\n",
    "        predictions[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Encontrando el mejor algoritmo y sus mejores hiperparametros\n",
    "\n",
    "1. Usando Random Forest encontramos cuanto aporta cada feature. Ordenamos en orden de importancia\n",
    "\n",
    "2. Generamos una lista de listas acumulativas de los features\n",
    "\n",
    "3. Corremos todos los algoritmos, entrenando con X_train, sobre todas las listas, en busqueda del mayor AUC\n",
    "\n",
    "4. Una vez encontrado el mejor algoritmo, se submitea entrenando con todo X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = SF.fr1_extract_X_y(df_users, df_y)\n",
    "X_train, X_test, y_train, y_test = SF.fr2_train_test_split(X, y, seed, test_size)\n",
    "model = random_forest(X_train, y_train, seed)\n",
    "X_to_predict, predictions = SF.fr6_extract_X_to_predict(df_users, df_y, model)\n",
    "feature_importances = SF.fr7_print_information(df_users, model, X_to_predict, True)\n",
    "\n",
    "display(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ordenados = feature_importances.index.tolist()\n",
    "lista_progresiva_de_cols = [features_ordenados[:i] for i in range(1,len(features_ordenados))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posibilidades = [('xgboost',xgboost),\n",
    "                 ('random_forest',random_forest),\n",
    "                 ('decision_tree',decision_tree),\n",
    "                 (f'KNN{K}',knn)\n",
    "                ]\n",
    "\n",
    "max_auc = 0\n",
    "campeon_indice = 0\n",
    "campeon_nombre = ''\n",
    "campeon_algoritmo = None\n",
    "\n",
    "for i, cols in enumerate(lista_progresiva_de_cols):\n",
    "    display(f'Iteracion {i} de {len(lista_progresiva_de_cols)}')\n",
    "    for nombre,algoritmo in posibilidades:\n",
    "        auc = full_framework_wrapper(nombre,algoritmo,columns=cols)\n",
    "        if auc>max_auc:\n",
    "            max_auc = auc\n",
    "            campeon_indice = i\n",
    "            campeon_nombre = nombre\n",
    "            campeon_algoritmo = algoritmo\n",
    "        \n",
    "display(f\"Mejor Apuesta: {campeon_nombre} ({max_auc:.2f} AUC), usando lista_progresiva_de_cols[{campeon_indice}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lista_progresiva_de_cols[campeon_indice])\n",
    "\n",
    "auc, csv_name = full_framework_wrapper(campeon_nombre+'_all_in',\n",
    "                       campeon_algoritmo,\n",
    "                       columns=lista_progresiva_de_cols[campeon_indice],\n",
    "                       submit=True,\n",
    "                       all_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descomentar y submitear!\n",
    "## Ojo, solo correr una vez!!!\n",
    "\n",
    "%env FILE_NAME = {csv_name}\n",
    "%env MESSAGE = Features: Lista acumulativa de {len(lista_progresiva_de_cols)} features ordenados por importancia, indice:{campeon_indice}.\n",
    "\n",
    "# !kaggle competitions submit -f $FILE_NAME -m \"$MESSAGE\" trocafone\n",
    "\n",
    "print('https://www.kaggle.com/c/trocafone/submissions?sortBy=publicScore&group=all&page=1&pageSize=20')\n",
    "print('https://www.kaggle.com/c/trocafone/leaderboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Lista de robos\n",
    "\n",
    "TODO: Borrar\n",
    "\n",
    "* [ ] https://github.com/urielkelman/abracadata/tree/master/TP2\n",
    "\n",
    "* [ ] https://github.com/MatiasReimondo/Datos\n",
    "\n",
    "* [ ] Ver diapos argerich feature engineering\n",
    "\n",
    "* [ ] Spammear ramos mejia de preguntas\n",
    "\n",
    "* [ ] Notebooks de Martin\n",
    "\n",
    "* [ ] Notebook de Juan\n",
    "\n",
    "## Lista de cosas a hacer\n",
    "\n",
    "* [ ] Identificar bias y varianza, ploteando error de set de entrenamiento y error de set de test en funcion de cantidad de datos en set de entrenamiento (mismo plot)\n",
    "\n",
    "* [ ] Perturbar datos de entrada -> reducir overfitting\n",
    "\n",
    "* [ ] Plotear AUC nuestra, AUC kaggle segun submission\n",
    "\n",
    "* [ ] Catboost\n",
    "\n",
    "* [ ] Loopear interacciones importantes! Random forest claveee\n",
    "\n",
    "* [ ] No perder nombre de feature original\n",
    "\n",
    "* [ ] Scrapear trocafone --> Precios\n",
    "\n",
    "* [ ] Xgboost no sirve para importancia de features. No es estable\n",
    "\n",
    "* [ ] Reclamar 50 usd aws &#128544; \n",
    "\n",
    "* [ ] Clustering para nuevos features para entrenar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contenido",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
