{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "# Notebook Principal\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "- 101055 - Bojman, Camila\n",
    "- 100029 - del Mazo, Federico\n",
    "- 100687 - Hortas, Cecilia\n",
    "- 97649 - Souto, Rodrigo\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**\n",
    "\n",
    "**https://www.kaggle.com/datatouille2018/competitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando la investigación sobre la empresa Trocafone realizada en el [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html), se busca determinar la probabilidad de que un usuario del sitio realice una conversión en el período determinado.\n",
    "\n",
    "Notebooks en orden de corrida y lectura:\n",
    "\n",
    "0. [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html) --> Familiarización con el set de datos y exploración de estos.\n",
    "\n",
    "1. [Investigación Previa](https://fdelmazo.github.io/7506-Datos/TP2/investigacion.html) --> Con ayuda de lo trabajado en el TP1, se averiguan más cosas de las datos, en busqueda de que poder reutilizar.\n",
    "\n",
    "2. [Creación de Dataframes](https://fdelmazo.github.io/7506-Datos/TP2/new-dataframes.html) --> Como parte del feature engineering, se crean dataframes nuevos con información de los productos del sitio y de como se accede a este (marcas, sistemas operativos, etc).\n",
    "\n",
    "3. [Feature Engineering](https://fdelmazo.github.io/7506-Datos/TP2/feature-engineering.html) -> Busqueda de atributos de los usuarios de los cuales se busca predecir la conversión\n",
    "\n",
    "4. Algoritmos de clasificación (directorio `AlgoritmosClasificacion`) --> Definiciones de algoritmos de machine learning para utilizar en este notebook\n",
    "\n",
    "  * [Árboles de decisión](https://fdelmazo.github.io/7506-Datos/TP2/AlgoritmosClasificacion/decision-tree.html): TODO, descr\n",
    "  * [Random Forest](https://fdelmazo.github.io/7506-Datos/TP2/AlgoritmosClasificacion/random-forest.html): TODO, descr\n",
    "  * [XGBoost](https://fdelmazo.github.io/7506-Datos/TP2/AlgoritmosClasificacion/xgboost.html): TODO, descr\n",
    "\n",
    "5. TP2 --> Teniendo todo en cuenta, juntar todos los datos buscados y encontrados sobre un mismo dataframe, aplicarle todos los algoritmos definidos previamente e ir realizando los entrenamientos y posteriores predicciones de conversiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up inicial, se deja comentado para evitar instalarle módulos al usuario\n",
    "\n",
    "## Primero, descargar los datasets de no tenerlos\n",
    "\n",
    "### Antes de comenzar, buscar las credenciales de kaggle (el nombre de usuario, y el token) y setearlos en las siguientes dos lineas\n",
    "### %env KAGGLE_USERNAME=\"datatouille2018\"\n",
    "### %env KAGGLE_KEY=\"xxx\"\n",
    "\n",
    "### !pip install kaggle # https://github.com/Kaggle/kaggle-api\n",
    "### !kaggle competitions download -c trocafone -p data\n",
    "### !unzip -q data/events_up_to_01062018.csv.zip -d data\n",
    "### !rm data/events_up_to_01062018.csv.zip\n",
    "### !ls data/\n",
    "\n",
    "## Luego, descargar los módulos a utilizar a lo largo de todo el trabajo\n",
    "\n",
    "### !pip install nbimporter\n",
    "### !conda install -c conda-forge xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from AlgoritmosClasificacion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.read_csv('data/events_up_to_01062018.csv', low_memory=False)\n",
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False)\n",
    "df_sessions = pd.read_csv('data/sessions.csv', low_memory=False)\n",
    "df_brands = pd.read_csv('data/brands.csv')\n",
    "df_os = pd.read_csv('data/os.csv')\n",
    "df_browsers = pd.read_csv('data/browsers.csv')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv')\n",
    "df_y = df_y.groupby('person').sum()\n",
    "\n",
    "df = df_events.merge(df_sessions, how='left', left_index=True, right_index=True)\n",
    "df = df.merge(df_browsers, how='left', on='browser_version')\n",
    "df = df.merge(df_os, how='left', on='operating_system_version')\n",
    "df = df.merge(df_brands, how='left', on='model')\n",
    "df = df.merge(df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos con pocos valores posibles se pasan a variables categoricas para ahorrar memoria\n",
    "df['event'] = df['event'].astype('category')\n",
    "df['condition'] = df['condition'].astype('category')\n",
    "df['storage'] = df['storage'].astype('category')\n",
    "df['search_engine'] = df['search_engine'].astype('category')\n",
    "df['channel'] = df['channel'].astype('category')\n",
    "df['device_type'] = df['device_type'].astype('category')\n",
    "\n",
    "df['brand'] = df['brand'].astype('category')\n",
    "df['operating_system'] = df['operating_system'].astype('category')\n",
    "df['browser'] = df['browser'].astype('category')\n",
    "\n",
    "# El tiempo es mejor manejarlo como tal\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df['month_number'] = df['timestamp'].dt.month\n",
    "df['month_name'] = df['month_number'].apply(lambda x: calendar.month_abbr[x])\n",
    "df['week_day'] = df['timestamp'].dt.weekday\n",
    "df['week_number'] = df['timestamp'].dt.week\n",
    "df['week_day_name'] = df['timestamp'].dt.weekday_name\n",
    "df['day_date'] = df['timestamp'].dt.to_period('D')\n",
    "df['day_dom'] = df['timestamp'].dt.day\n",
    "df['day_doy'] = df['timestamp'].dt.dayofyear\n",
    "df['hour_count'] = df['timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,:-1],data.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def require(x1, x2):\n",
    "    if x1 != x2:\n",
    "        print('ERROR - {} must be equal to {}'.format(str(x1), str(x2)))\n",
    "        raise ValueError('Oh la la.') \n",
    "\n",
    "def df_label_xor(df1, df2):\n",
    "    \n",
    "    merged = df1.merge(df2, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    merged = merged.query('_merge != \"both\"')\n",
    "    return merged\n",
    "    \n",
    "# Crea la matriz X y el vector y para entrenar\n",
    "def fr1_extract_X_y(df, df_y):\n",
    "    require(len(df), 38829)\n",
    "    require(len(df_y), 19414)\n",
    "    \n",
    "    data = df.merge(df_y, how='inner', left_index= True, right_index=True)\n",
    "    require(len(data), 19414)\n",
    "    \n",
    "    X = data.drop('label', axis=1).values\n",
    "    y = df_y.values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Splitea para generar los set de entrenamiento y de prueba\n",
    "def fr2_train_test_split(X, y, seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.34,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Customizado, se tiene que hacer uno por algoritmo\n",
    "def fr3_decision_tree(X_train, X_test, y_train, seed):\n",
    "    tree = DecisionTreeClassifier(criterion='gini',\n",
    "                              min_samples_leaf=5,\n",
    "                              min_samples_split=5,\n",
    "                              max_depth=3,\n",
    "                              random_state=seed)\n",
    "\n",
    "    tree.fit(X_train, y_train)\n",
    "    return tree\n",
    "\n",
    "def fr3_random_forest(X_train, X_test, y_train, seed):\n",
    "    rf = RandomForestClassifier(n_estimators=20,\n",
    "                           n_jobs=2,\n",
    "                           min_samples_split=300,\n",
    "                           random_state=seed,\n",
    "                           class_weight='balanced')\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "    \n",
    "def fr4_accuracy_score(X_test, y_test, model, model_name):\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('{} accuracy score: {}'.format(model_name, accuracy))    \n",
    "    \n",
    "# Crea la matriz X a predecir\n",
    "def fr5_extract_X_to_predict(df, df_y, model):\n",
    "    require(len(df), 38829)\n",
    "    require(len(df_y), 19414)\n",
    "       \n",
    "    data = df_label_xor(df, df_y)\n",
    "    data = data.drop(['label', '_merge'], axis=1)\n",
    "\n",
    "    require(len(data), 19415)\n",
    "\n",
    "    predictions = model.predict(data.values)\n",
    "    return data, predictions\n",
    "\n",
    "# Devuelve la cantidad de 1s predecidos\n",
    "def fr6_print_information(df, predictions, model):\n",
    "    print('Ammount of 1s: {}'.format(predictions.sum()))\n",
    "    \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                  index=df.columns,\n",
    "                                  columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "    display(feature_importances)\n",
    "    \n",
    "    \n",
    "def fr7_to_csv(df, predictions, name_csv):\n",
    "    submission = df\n",
    "    submission['label'] = predictions\n",
    "    submission = submission['label']\n",
    "    \n",
    "    require(len(submission), 19415)\n",
    "    \n",
    "    submission.to_csv(name_csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummys de todos 0s y todos 1s\n",
    "gb = df[['person', 'week_number']].groupby('person')\n",
    "dummy = gb.agg('sum')\n",
    "\n",
    "dummy = dummy['week_number'] * 0\n",
    "dummy = dummy.to_frame()\n",
    "print(\"dummy-size: \"+str(len(dummy))+\" | y-size: \"+str(len(df_y)))\n",
    "y_df_tmp = df_y.set_index(df_y.index)[['label']]\n",
    "\n",
    "dummy_final = dummy.merge(y_df_tmp, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "#display(dummy.head())\n",
    "#display(y_df.head())\n",
    "#display(dummy_final.head())\n",
    "dummy['label'] = dummy['week_number'] * 0\n",
    "dummy = dummy[['label']]\n",
    "dummy = dummy[~dummy.index.isin(df_y.index)]\n",
    "display(len(dummy))\n",
    "dummy\n",
    "display(len(dummy_final.query('_merge != \"both\"')))\n",
    "dummy_final = dummy_final.query('_merge != \"both\"')[['week_number']]\n",
    "dummy_final.columns = ['label']\n",
    "dummy_final['label'] = dummy_final['label'] + 1\n",
    "print(\"checking sum is zero: \")\n",
    "display(dummy_final.sum())\n",
    "dummy_final.head()\n",
    "\n",
    "dummy_final.to_csv('submit-zeros.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlgoritmosClasificacion.xgboost.XGboost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lista de robos\n",
    "\n",
    "TODO: Borrar\n",
    "\n",
    "* [ ] https://github.com/urielkelman/abracadata/tree/master/TP2\n",
    "\n",
    "* [ ] https://github.com/GastonMontes/Datos-TP2\n",
    "\n",
    "* [ ] https://github.com/MatiasReimondo/Datos\n",
    "\n",
    "* [ ] Ver diapos argerich feature engineering\n",
    "\n",
    "* [ ] Spammear ramos mejia de preguntas\n",
    "\n",
    "* [ ] Notebooks de Martin\n",
    "\n",
    "* [ ] Notebook de Juan\n",
    "\n",
    "### Lista de cosas a hacer\n",
    "\n",
    "* [ ] Dividir en set de entrenamientos y set de test. Ojo, hay que hacerlo con tiempo! No se puede hacer al azar. Los primeros meses entrenan a los siguientes. Etc\n",
    "\n",
    "* [ ] Identificar bias y varianza, ploteando error de set de entrenamiento y error de set de test en funcion de cantidad de datos en set de entrenamiento (mismo plot)\n",
    "\n",
    "* [ ] Perturbar datos de entrada -> reducir overfitting\n",
    "\n",
    "* [ ] Feature engineering --> no olvidar documentar y versionar!!!\n",
    "\n",
    "* [ ] Catboost\n",
    "\n",
    "* [ ] Feature: dia de la semana\n",
    "\n",
    "* [ ] Loopear interacciones importantes! Random forest claveee\n",
    "\n",
    "* [ ] No perder nombre de feature original\n",
    "\n",
    "* [ ] Scrapear trocafone --> Precios\n",
    "\n",
    "* [ ] Xgboost no sirve para importancia de features. No es estable\n",
    "\n",
    "* [ ] Reclamar 50 usd aws &#128544; \n",
    "\n",
    "* [ ] Clustering para nuevos features para entrenar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contenido",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.867px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
