{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "# Notebook Principal\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "- 101055 - Bojman, Camila\n",
    "- 100029 - del Mazo, Federico\n",
    "- 100687 - Hortas, Cecilia\n",
    "- 97649 - Souto, Rodrigo\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**\n",
    "\n",
    "**https://www.kaggle.com/datatouille2018/competitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando la investigación sobre la empresa Trocafone realizada en el [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html), se busca determinar la probabilidad de que un usuario del sitio realice una conversión en el período determinado.\n",
    "\n",
    "Notebooks en orden de corrida y lectura:\n",
    "\n",
    "0. [TP1](https://fdelmazo.github.io/7506-Datos/TP1/TP1.html) --> Familiarización con el set de datos y exploración de estos.\n",
    "\n",
    "1. [Investigación Previa](https://fdelmazo.github.io/7506-Datos/TP2/investigacion.html) --> Con ayuda de lo trabajado en el TP1, se averiguan más cosas de las datos, en busqueda de que poder reutilizar.\n",
    "\n",
    "2. [Creación de Dataframes](https://fdelmazo.github.io/7506-Datos/TP2/new-dataframes.html) --> Como parte del feature engineering, se crean dataframes nuevos con información de los productos del sitio y de como se accede a este (marcas, sistemas operativos, etc).\n",
    "\n",
    "3. [Feature Engineering](https://fdelmazo.github.io/7506-Datos/TP2/feature-engineering.html) --> Busqueda de atributos de los usuarios de los cuales se busca predecir la conversión.\n",
    "\n",
    "4. [Submission Framework](https://fdelmazo.github.io/7506-Datos/TP2/submission-framework.html) --> Pequeño framework para construir las postulaciones de labels. \n",
    "\n",
    "5. TP2 (este notebook)--> Teniendo todo en cuenta, usando los dataframes con todos los atributos buscados y encontrados, se definen y aplican los algoritmos de clasificación, se realizan los entrenamientos y posteriores predicciones de conversiones y finalmente se arman las postulaciones de labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up inicial, se deja comentado para evitar instalarle módulos al usuario\n",
    "## Primero, descargar los datasets de no tenerlos\n",
    "\n",
    "# Antes de comenzar, setear las credenciales (usuario y token)\n",
    "\n",
    "# 1. Visitar: https://www.kaggle.com/datatouille2018/account (con la cuenta que sea)\n",
    "# 2. Tocar en Create New API Token\n",
    "# 3. Guardar el archivo descargado en ~/.kaggle/kaggle.json\n",
    "\n",
    "# !pip install kaggle # https://github.com/Kaggle/kaggle-api\n",
    "# !kaggle competitions download -c trocafone -p data\n",
    "# !unzip -q data/events_up_to_01062018.csv.zip -d data\n",
    "# !rm data/events_up_to_01062018.csv.zip\n",
    "# !ls data/\n",
    "\n",
    "## Luego, descargar los módulos a utilizar a lo largo de todo el trabajo\n",
    "\n",
    "# !pip install nbimporter\n",
    "# !conda install -c conda-forge xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter # pip install nbimporter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import submission_framework as SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False).set_index('person')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv').groupby('person').sum()\n",
    "\n",
    "display(df_users.head(), df_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.34\n",
    "\n",
    "def full_framework_wrapper(model_name, model_function, columns=df_users.columns.tolist(), test_size=test_size, seed=seed, verbosity=0, all_in=False, submit=False,feature_importance_print=False):\n",
    "    model_df_x = df_users[columns]\n",
    "    model_df_y = df_y\n",
    "    \n",
    "    X, y = SF.fr1_extract_X_y(model_df_x, model_df_y)\n",
    "    X_train, X_test, y_train, y_test = SF.fr2_train_test_split(X, y, seed, test_size)\n",
    "    if all_in:\n",
    "        model = model_function(X, y, seed)\n",
    "    else:\n",
    "        model = model_function(X_train, y_train, seed)\n",
    "    accuracy = SF.fr4_accuracy_score(X_test, y_test, model, model_name)\n",
    "    auc = SF.fr5_auc(X_test, y_test, model, model_name)\n",
    "    X_to_predict, predictions = SF.fr6_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "       \n",
    "    if verbosity>=1: print('Model: {} - Columns: {} (AUC: {:.4f})'.format(model_name, columns, auc))  \n",
    "    elif verbosity >=0: print('Model: {} - Accuracy: {:.4f} - AUC: {:.4f}'.format(model_name, accuracy, auc))  \n",
    "    \n",
    "    if feature_importance_print: \n",
    "        feature_importances = SF.fr7_print_information(model_df_x, model, X_to_predict, True)\n",
    "        display(feature_importances)\n",
    "            \n",
    "    if submit:\n",
    "        csv_name, submission = SF.fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "        display(csv_name)\n",
    "        return auc, csv_name\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "def decision_tree(X_train, y_train, seed):\n",
    "    tree = DecisionTreeClassifier(criterion='gini',\n",
    "                              min_samples_leaf=5,\n",
    "                              min_samples_split=5,\n",
    "                              max_depth=3,\n",
    "                              random_state=seed)\n",
    "\n",
    "    tree.fit(X_train, y_train)\n",
    "    return tree\n",
    "\n",
    "full_framework_wrapper('decision_tree',decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest(X_train, y_train, seed):\n",
    "    rf = RandomForestClassifier(n_estimators=20,\n",
    "                           n_jobs=2,\n",
    "                           min_samples_split=300,\n",
    "                           random_state=seed,\n",
    "                           class_weight='balanced')\n",
    "\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "\n",
    "full_framework_wrapper('random_forest',random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DAR BOLA POR AHORA\n",
    "\n",
    "# No era necesario hacer el entrenamiento final con el modelo\n",
    "# entrenado con X_train. Conviene usar el set de entrenamiento\n",
    "# entero.\n",
    "\n",
    "# Lo de X_train y X_test es sólo (POR AHORA) para mostrar\n",
    "# el accuracy_score choto que estamos teniendo\n",
    "\n",
    "full_framework_wrapper('random_forest_allin',random_forest,all_in=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_name = 'knn'\n",
    "\n",
    "rango = 5\n",
    "def knn_gridsearch(x_train, y_train, x_test, y_test):\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    k_max = (0,0)\n",
    "    for k in range(1,100, rango):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "        knn.fit(x_train, y_train)\n",
    "        score = knn.score(x_test, y_test)*100\n",
    "        if score > k_max[1]:\n",
    "            k_max = (k,score)\n",
    "        print(\"K: {}, {}\".format(k, score))\n",
    "    a = k_max[0]\n",
    "    k_max = (0,0)\n",
    "    for k in range(a - rango, a + rango):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "        knn.fit(x_train, y_train)\n",
    "        score = knn.score(x_test, y_test)*100\n",
    "        if score > k_max[1]:\n",
    "            k_max = (k,score)\n",
    "        print(\"K: {}, {}\".format(k, score))\n",
    "    return k_max[0]\n",
    "\n",
    "X, y = SF.fr1_extract_X_y(df_users, df_y)\n",
    "X_train, X_test, y_train, y_test = SF.fr2_train_test_split(X, y, seed, test_size)    \n",
    "K = knn_gridsearch(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def knn(X_train, y_train, seed, k=K):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', n_jobs=-1)\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "full_framework_wrapper(f'KNN{K}',knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb #conda install -c conda-forge xgboost \n",
    "\n",
    "def xgboost(X_train, y_train, seed):\n",
    "    xg_reg = xgb.XGBClassifier(objective='reg:linear',\n",
    "                class_weight='balanced',\n",
    "                colsample_bytree=0.3, learning_rate=0.1,\n",
    "                min_child_weight=2,\n",
    "                max_depth=5, alpha=10, n_estimators=20,\n",
    "                random_state=seed)\n",
    "    y_train.shape = y_train.shape[0]\n",
    "    xg_reg.fit(X_train,y_train)\n",
    "    return xg_reg\n",
    "\n",
    "full_framework_wrapper('xgboost',xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NO BORRAR ESTO !!!! ok, lo comento ;)\n",
    "\n",
    "# total_predictions = 0\n",
    "# tmp_seed = seed\n",
    "# for i in range(50):\n",
    "#     tmp_seed = tmp_seed + i\n",
    "#     model = xgboost(X, y, tmp_seed)\n",
    "#     print(tmp_seed)\n",
    "#     X_to_predict, predictions = SF.fr6_extract_X_to_predict(fr_df, fr_df_y, model)\n",
    "#     total_predictions = total_predictions + predictions\n",
    "\n",
    "# predictions = total_predictions / 50\n",
    "\n",
    "# \"\"\"\n",
    "# model = xgboost(X, y, seed)\n",
    "# X_to_predict, predictions = SF.fr6_extract_X_to_predict(fr_df, fr_df_y, model)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(predictions)):\n",
    "#     if predictions[i] < 0:\n",
    "#         print(\"WARNING: prediction[{}] = {}\".format(i, predictions[i]))\n",
    "#         predictions[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Encontrando el mejor submit\n",
    "\n",
    "Usando Random Forest, el algoritmo más estable de los definidos (XGBoost es poco estable) encontramos que combinación de features es la mas favorable (con la métrica Area Under Curve).\n",
    "\n",
    "Corremos todos los algoritmos definidos sobre esas combinaciones, en busqueda de su mejor combinación de hiper-parametros.\n",
    "\n",
    "Finalmente, se corren todos los algoritmos en su mejor combinación contra todos los set de features definidos, en busqueda de la mejor fusión universal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "TODO: Pasar a un notebook nuevo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Importance\n",
    "\n",
    "Se parte de una lista de todos los features ordeandos segun importancia, y se genera una lista de listas acumulativa de esto. Es decir de `[a,b,c]` se pasa a `[ [a], [a,b], [a,b,c] ]`\n",
    "\n",
    "Esto se hace porque se esta buscando el 'codo': Los features que hacen que incremente el AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = SF.fr1_extract_X_y(df_users, df_y)\n",
    "X_train, X_test, y_train, y_test = SF.fr2_train_test_split(X, y, seed, test_size)\n",
    "model = random_forest(X_train, y_train, seed)\n",
    "X_to_predict, predictions = SF.fr6_extract_X_to_predict(df_users, df_y, model)\n",
    "feature_importances = SF.fr7_print_information(df_users, model, X_to_predict, True)\n",
    "\n",
    "features_ordenados = feature_importances.index.tolist()\n",
    "lista_progresiva_de_cols = [features_ordenados[:i] for i in range(1,len(features_ordenados))]\n",
    "\n",
    "max_auc = full_framework_wrapper('random_forest',random_forest)\n",
    "best_features_progresivo = features_ordenados\n",
    "features_con_saltos_progresivo = []\n",
    "\n",
    "for i, cols in enumerate(lista_progresiva_de_cols):\n",
    "    print(f'\\n\\nIteración {i} de {len(lista_progresiva_de_cols)}\\n\\n')\n",
    "    auc = full_framework_wrapper('random_forest',random_forest,columns=cols,verbosity=1)\n",
    "    if auc > max_auc + 0.0001:\n",
    "        max_auc = auc\n",
    "        best_features_progresivo = cols\n",
    "        features_con_saltos_progresivo.append(cols[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features_con_saltos_progresivo,best_features_progresivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Forward Selection\n",
    "\n",
    "Se parte de una lista vacía y se van agregando todos los features uno por uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_users.columns.tolist()\n",
    "\n",
    "cantidad_features = len(features)\n",
    "\n",
    "max_auc = 0\n",
    "features_no_usadas = features\n",
    "features_ya_usadas = []\n",
    "best_features_forward = []\n",
    "features_con_saltos_forward = []\n",
    "\n",
    "for i in range(cantidad_features+1):\n",
    "    print(f'\\n\\nIteración {i} de {cantidad_features}')\n",
    "    print(f'Lo mejor al momento: {best_features_forward} (AUC: {max_auc:.4f}) \\n\\n')\n",
    "    max_local_auc = 0\n",
    "    for f in features_no_usadas:\n",
    "        features_a_usar = features_ya_usadas + [f]\n",
    "        auc = full_framework_wrapper('random_forest',random_forest,columns=features_a_usar,verbosity=1)\n",
    "        if auc > max_local_auc + 0.0001:\n",
    "            max_local_auc = auc\n",
    "            feature_a_agregar = f\n",
    "        \n",
    "    features_ya_usadas += [feature_a_agregar]\n",
    "    if features_no_usadas: features_no_usadas.remove(feature_a_agregar)\n",
    "    \n",
    "    if max_local_auc > max_auc + 0.0001:\n",
    "        max_auc = max_local_auc\n",
    "        best_features_forward = features_ya_usadas[:]\n",
    "        features_con_saltos_forward.append(feature_a_agregar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(features_con_saltos_forward,best_features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Backward Elimination\n",
    "\n",
    "Se parte de una lista con todos los features y se van sacando uno por uno, en busqueda de cual hace que incremente un AUC una vez que se lo remueva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_users.columns.tolist()\n",
    "cantidad_features = len(features)\n",
    "\n",
    "max_auc = full_framework_wrapper('random_forest',random_forest,verbosity=1)\n",
    "max_local_auc = max_auc\n",
    "best_features_backward = features\n",
    "features_ya_usadas = features\n",
    "features_con_saltos_backward = []\n",
    "\n",
    "for i in range(cantidad_features+1):\n",
    "    print(f'\\n\\nIteración {i} de {cantidad_features}')\n",
    "    print(f'Lo mejor al momento: {best_features_backward} (AUC: {max_auc:.4f}) \\n\\n')\n",
    "    for f in features_ya_usadas:\n",
    "        features_a_usar = features_ya_usadas[:]\n",
    "        features_a_usar.remove(f)\n",
    "        auc = full_framework_wrapper('random_forest',random_forest,columns=features_a_usar,verbosity=1)\n",
    "        if auc > max_local_auc + 0.0001:\n",
    "            max_local_auc = auc\n",
    "            feature_a_borrar = f\n",
    "            \n",
    "    if feature_a_borrar in features_ya_usadas: features_ya_usadas.remove(feature_a_borrar)\n",
    "        \n",
    "    if max_local_auc > max_auc + 0.0001:\n",
    "        max_auc = max_local_auc\n",
    "        best_features_backward = features_ya_usadas[:]\n",
    "        features_con_saltos_backward.append(feature_a_borrar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tener en cuenta que en este caso, features_con_saltos_backward se refiere a aquellos features que al removerlos se ve un incremento. Es decir, son los features que menos sirven, correr el algoritmo solo sobre esto tendría resultados muy poco favorables.')\n",
    "display(features_con_saltos_backward,best_features_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_cols_1 = ['total_checkouts_month_5',\n",
    "'timestamp_last_checkout',\n",
    "'timestamp_last_event',\n",
    "'has_checkout_month_5',\n",
    "'total_checkouts',\n",
    "'days_to_last_event',\n",
    "'total_checkouts_last_week',\n",
    "'total_checkouts_months_1_to_4',\n",
    "'total_conversions',\n",
    "'total_session_conversions',\n",
    "'total_events',\n",
    "'total_sessions',\n",
    "'avg_events_per_session',\n",
    "'total_session_checkouts',\n",
    "'has_checkout']\n",
    "\n",
    "posibilidades_features = {\n",
    "    'Best Cumulative Importance':best_features_progresivo,\n",
    "    'Best Forward Selection':best_features_forward,\n",
    "    'Best Backward Elimination':best_features_backward,\n",
    "    'Leap Cumulative Importance':features_con_saltos_progresivo,\n",
    "    'Leap Forward Selection':features_con_saltos_forward,\n",
    "    'Selección a Mano': ss_cols_1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando los mejores hiper-parametros para cada algoritmo\n",
    "\n",
    "Con Grid Search se busca la mejor combinación de hiperparametros para cada algoritmo, haciendo un promedio de como performan contra cada uno de los `best_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GRID SEARCH SOBRE TODOS LOS ALGORITMOS Y TODOS SUS HIPERPARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posibilidades_algoritmos = [('xgboost',xgboost),\n",
    "                 ('random_forest',random_forest),\n",
    "                 ('decision_tree',decision_tree),\n",
    "                 (f'KNN{K}',knn)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando la mejor combinación\n",
    "\n",
    "Se corren todas las `best` version de cada algoritmo sobre todos los set de features importantes, en distintos ordenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANTIDAD_ORDENES = 10\n",
    "\n",
    "max_auc = 0\n",
    "campeon_nombre = ''\n",
    "campeon_algoritmo = None\n",
    "campeon_forma = None\n",
    "campeon_features = None\n",
    "\n",
    "for nombre,algoritmo in posibilidades_algoritmos:\n",
    "    for forma,features_seleccionadas in posibilidades_features.items():\n",
    "        features_nuevos_ordenes = [np.random.permutation(features_seleccionadas) for x in range(CANTIDAD_ORDENES)]\n",
    "        features_ordenes_posibles = features_nuevos_ordenes + [features_seleccionadas]\n",
    "        for features_con_orden in features_ordenes_posibles:\n",
    "            features = list(features_con_orden)\n",
    "            auc = full_framework_wrapper(nombre,algoritmo,columns=features)\n",
    "            if auc > max_auc:\n",
    "                max_auc = auc\n",
    "                campeon_nombre = nombre\n",
    "                campeon_algoritmo = algoritmo\n",
    "                campeon_forma = forma\n",
    "                campeon_features = features\n",
    "            \n",
    "display(f\"Mejor Apuesta: {campeon_nombre} ({max_auc:.2f} AUC) - Features: {campeon_forma}\")\n",
    "display(f\"Orden Features: {campeon_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Encontrar mejor forma de ensamblar y de boostear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrida Final\n",
    "\n",
    "Se corre entrenando con X (y no X_train) el submit final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc, csv_name = full_framework_wrapper(campeon_nombre+'_all_in',\n",
    "                       campeon_algoritmo,\n",
    "                       columns=campeon_features,\n",
    "                       submit=True,\n",
    "                       verbosity=1,\n",
    "                       all_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descomentar y submitear!\n",
    "## Ojo, solo correr una vez!!!\n",
    "%env FILE_NAME = {csv_name}\n",
    "%env MESSAGE = {campeon_nombre}_all_in - {campeon_forma} - {campeon_features}\n",
    "\n",
    "#!kaggle competitions submit -f $FILE_NAME -m \"$MESSAGE\" trocafone\n",
    "\n",
    "print()\n",
    "print('https://www.kaggle.com/c/trocafone/submissions?sortBy=date')\n",
    "print('https://www.kaggle.com/c/trocafone/leaderboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Algoritmos y Features\n",
    "\n",
    "* PRECIOS\n",
    "\n",
    "* Naive Bayes\n",
    "\n",
    "* Perceprton\n",
    "\n",
    "* SVM\n",
    "\n",
    "* https://github.com/urielkelman/abracadata/tree/master/TP2\n",
    "\n",
    "* https://github.com/MatiasReimondo/Datos\n",
    "\n",
    "\n",
    "## Lista de cosas que se pueden hacer\n",
    "\n",
    "* [ ] Identificar bias y varianza, ploteando error de set de entrenamiento y error de set de test en funcion de cantidad de datos en set de entrenamiento (mismo plot)\n",
    "\n",
    "* [ ] Perturbar datos de entrada -> reducir overfitting\n",
    "\n",
    "* [ ] Plotear AUC nuestra, AUC kaggle segun submission\n",
    "\n",
    "* [ ] Catboost\n",
    "\n",
    "* [ ] Reclamar 50 usd aws &#128544; \n",
    "\n",
    "* [ ] Clustering para nuevos features para entrenar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contenido",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
