{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "# Parameter Tuning\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Siendo este un proceso muy costoso en tiempo, se corre una vez y se guardan los resultados para luego poder importarlos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hiper_params():\n",
    "    # Se esconde en un def para poder importarlo\n",
    "\n",
    "    return {\n",
    "        'decision_tree':{'criterion': 'gini',\n",
    "                         'max_features': 0.30000000000000004,\n",
    "                         'max_depth': 8.75,\n",
    "                         'min_samples_split': 0.1,\n",
    "                         'min_samples_leaf': 0.0001},\n",
    "\n",
    "        'random_forest': {'n_estimators': 200,\n",
    "                         'criterion': 'entropy',\n",
    "                         'max_features': 0.2,\n",
    "                         'max_depth': 16.5,\n",
    "                         'min_samples_split': 0.1,\n",
    "                         'min_samples_leaf': 0.1},\n",
    "        \n",
    "        # AUC: 0.8695\n",
    "        'xgboost': {'learning_rate': 0.1,\n",
    "                     'objective': 'binary:logistic',\n",
    "                     'n_estimators': 16,\n",
    "                     'scale_pos_weight': 2,\n",
    "                     'max_depth': 4,\n",
    "                     'min_child_weight': 5,\n",
    "                     'gamma': 0.0,\n",
    "                     'colsample_bytree': 0.7500000000000001,\n",
    "                     'subsample': 0.7,\n",
    "                     'colsample_bylevel': 0.65},\n",
    "        \n",
    "        'knn': {'n_neighbors':21, 'weights':'uniform', 'n_jobs':-1},\n",
    "                \n",
    "        # AUC: 0.8700\n",
    "        'lightgbm': {'objective': 'binary',\n",
    "                     'num_leaves': 36,\n",
    "                     'n_estimators': 70,\n",
    "                     'min_split_gain': 0.01,\n",
    "                     'min_child_weight': 5.00001,\n",
    "                     'max_depth': 4,\n",
    "                     'learning_rate': 0.05,\n",
    "                     'lambda_l2': 0,\n",
    "                     'feature_fraction': 0.7000000000000001,\n",
    "                     'bagging_fraction': 1.0},\n",
    "\n",
    "        'catboost': { 'eval_metric': 'AUC',\n",
    "                     'iterations': 678,\n",
    "                     'random_strength': 42,\n",
    "                     'learning_rate': 0.01,\n",
    "                     'depth': 1,\n",
    "                     'l2_leaf_reg': 2},\n",
    "        \n",
    "        'gradient_boosting': {'max_leaf_nodes': None,\n",
    "                     'min_weight_fraction_leaf': 0,\n",
    "                     'learning_rate': 0.1,\n",
    "                     'max_features': 1,\n",
    "                     'min_samples_split': 1.0,\n",
    "                     'min_samples_leaf': 0.1,\n",
    "                     'max_depth': 1.0,\n",
    "                     'n_estimators': 1,\n",
    "                     'subsample': 0.8,\n",
    "                     'loss': 'deviance',\n",
    "                     'warm_start': False,\n",
    "                     'presort': 'auto'},\n",
    "        \n",
    "        'neuralnetwork': {'hidden_layer_sizes': (4, 4),\n",
    "                         'activation': 'relu',\n",
    "                         'alpha': 0.0001,\n",
    "                         'beta_1': 0.05,\n",
    "                         'beta_2': 0.86,\n",
    "                         'early_stopping': False,\n",
    "                         'epsilon': 1e-08,\n",
    "                         'learning_rate': 'constant',\n",
    "                         'solver': 'adam',\n",
    "                         'validation_fraction': 0.15}\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from submission_framework.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter # pip install nbimporter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import submission_framework as SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False).set_index('person')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv').groupby('person').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_mano = [ 'total_checkouts',\n",
    " 'total_conversions',\n",
    " 'total_events',\n",
    " 'total_sessions',\n",
    " 'total_session_checkout',\n",
    " 'total_session_conversion',\n",
    " 'total_events_ad_session',\n",
    " 'total_ad_sessions',\n",
    " 'avg_events_per_session',\n",
    " 'avg_events_per_ad_session',\n",
    " 'percentage_session_ad',\n",
    " 'has_checkout',\n",
    " 'has_conversion',\n",
    " 'total_viewed_products_month_1',\n",
    " 'total_checkouts_month_1',\n",
    " 'total_conversions_month_1',\n",
    " 'total_events_month_1',\n",
    " 'total_sessions_month_1',\n",
    " 'total_session_checkouts_month_1',\n",
    " 'total_session_conversions_month_1',\n",
    " 'total_events_ad_session_month_1',\n",
    " 'total_ad_sessions_month_1',\n",
    " 'has_checkout_month_1',\n",
    " 'has_conversion_month_1',\n",
    " 'total_viewed_products_month_2',\n",
    " 'total_checkouts_month_2',\n",
    " 'total_conversions_month_2',\n",
    " 'total_events_month_2',\n",
    " 'total_sessions_month_2',\n",
    " 'total_session_checkouts_month_2',\n",
    " 'total_session_conversions_month_2',\n",
    " 'total_events_ad_session_month_2',\n",
    " 'total_ad_sessions_month_2',\n",
    " 'has_checkout_month_2',\n",
    " 'has_conversion_month_2',\n",
    " 'total_viewed_products_month_3',\n",
    " 'total_checkouts_month_3',\n",
    " 'total_conversions_month_3',\n",
    " 'total_events_month_3',\n",
    " 'total_sessions_month_3',\n",
    " 'total_session_checkouts_month_3',\n",
    " 'total_session_conversions_month_3',\n",
    " 'total_events_ad_session_month_3',\n",
    " 'total_ad_sessions_month_3',\n",
    " 'has_checkout_month_3',\n",
    " 'has_conversion_month_3',\n",
    " 'total_viewed_products_month_4',\n",
    " 'total_checkouts_month_4',\n",
    " 'total_conversions_month_4',\n",
    " 'total_events_month_4',\n",
    " 'total_sessions_month_4',\n",
    " 'total_session_checkouts_month_4',\n",
    " 'total_session_conversions_month_4',\n",
    " 'total_events_ad_session_month_4',\n",
    " 'total_ad_sessions_month_4',\n",
    " 'has_checkout_month_4',\n",
    " 'has_conversion_month_4',\n",
    " 'total_viewed_products_month_5',\n",
    " 'total_checkouts_month_5',\n",
    " 'total_conversions_month_5',\n",
    " 'total_events_month_5',\n",
    " 'total_sessions_month_5',\n",
    " 'total_session_checkouts_month_5',\n",
    " 'total_session_conversions_month_5',\n",
    " 'total_events_ad_session_month_5',\n",
    " 'total_ad_sessions_month_5',\n",
    " 'has_checkout_month_5',\n",
    " 'has_conversion_month_5',\n",
    " 'total_viewed_products_months_1_to_4',\n",
    " 'total_checkouts_months_1_to_4',\n",
    " 'total_conversions_months_1_to_4',\n",
    " 'total_events_months_1_to_4',\n",
    " 'total_sessions_months_1_to_4',\n",
    " 'total_session_checkouts_months_1_to_4',\n",
    " 'total_session_conversions_months_1_to_4',\n",
    " 'total_events_ad_session_months_1_to_4',\n",
    " 'total_ad_sessions_months_1_to_4',\n",
    " 'has_checkout_months_1_to_4',\n",
    " 'has_conversion_months_1_to_4',\n",
    " 'total_viewed_products_lw',\n",
    " 'total_checkouts_lw',\n",
    " 'total_conversions_lw',\n",
    " 'total_events_lw',\n",
    " 'total_sessions_lw',\n",
    " 'total_session_checkouts_lw',\n",
    " 'total_session_conversions_lw',\n",
    " 'total_events_ad_session_lw',\n",
    " 'total_ad_sessions_lw',\n",
    " 'has_checkout_lw',\n",
    " 'has_conversion_lw',\n",
    " 'amount_of_months_that_has_bought',\n",
    " 'timestamp_last_event',\n",
    " 'timestamp_last_checkout',\n",
    " 'timestamp_last_conversion',\n",
    " 'timestamp_last_viewed_product',\n",
    " 'days_to_last_event',\n",
    " 'days_to_last_checkout',\n",
    " 'days_to_last_conversion',\n",
    " 'days_to_last_viewed_product',\n",
    " 'doy_last_event',\n",
    " 'dow_last_event',\n",
    " 'dom_last_event',\n",
    " 'woy_last_event',\n",
    " 'doy_last_checkout',\n",
    " 'dow_last_checkout',\n",
    " 'dom_last_checkout',\n",
    " 'woy_last_checkout',\n",
    " 'doy_last_conversion',\n",
    " 'dow_last_conversion',\n",
    " 'dom_last_conversion',\n",
    " 'woy_last_conversion',\n",
    " 'doy_last_viewed_product',\n",
    " 'dow_last_viewed_product',\n",
    " 'dom_last_viewed_product',\n",
    " 'woy_last_viewed_product',\n",
    " 'last_conversion_sku',\n",
    " 'last_conversion_price',\n",
    " 'percentage_last_week_activity',\n",
    " 'percentage_last_month_activity',\n",
    " 'days_between_last_event_and_checkout',\n",
    " 'percentage_regular_celphones_activity',\n",
    " 'var_viewed',\n",
    " 'conversion_gt_media'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(df_x, df_y, orig_model_with_name, default_params, list_of_progressive_params,\n",
    "                     columns=None, seed=0, cv=5, normalize=False):\n",
    "    \n",
    "    \"\"\"Encuentra los mejores hiper-parametros con grid search pero de manera secuencial. \n",
    "    Es decir, en vez de probar 3 variables con 5 valores cada uno, lo cual resulta en 455 modelos, se puede partir en dos variables de 5 valores (25 modelos) y del mejor de eso los otros 5 valores (125 modelos en total)\n",
    "    Ciertamente, no es óptimo. Pero ahorra tiempo valioso\"\"\"\n",
    "    \n",
    "    orig_model_name, orig_model = orig_model_with_name\n",
    "    orig_model_name+='_GSF'\n",
    "    acc_params = {}\n",
    "    i=1\n",
    "    \n",
    "    for params_grid in list_of_progressive_params:\n",
    "        print(f\"Best Params So Far: {default_params} {acc_params}\\n\\n\")\n",
    "\n",
    "        model_new = GridSearchCV(orig_model(**default_params,**acc_params,random_state=seed) ,params_grid, cv=cv, verbose=1, scoring='roc_auc',n_jobs=4)\n",
    "   \n",
    "        model_with_name = (orig_model_name,model_new)\n",
    "        model, auc = SF.full_framework_wrapper(df_x, df_y, model_with_name, columns=columns, normalize=normalize)\n",
    "        acc_params.update(model.best_params_)\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    default_params.update(acc_params)\n",
    "    return default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_random(df_x, df_y, orig_model_with_name, default_params, params_grid,\n",
    "                            n_iter=15, columns=None, seed=0, cv=5):\n",
    "    \n",
    "    orig_model_name, orig_model = orig_model_with_name\n",
    "    orig_model_name+='_RS'\n",
    "    i=1\n",
    "\n",
    "    model_new = RandomizedSearchCV(orig_model(**default_params,random_state=seed), params_grid, n_iter=n_iter, cv=cv, verbose=1, scoring='roc_auc', n_jobs=4)\n",
    "\n",
    "    model_with_name = (orig_model_name,model_new)\n",
    "    model, auc = SF.full_framework_wrapper(df_x, df_y, model_with_name,columns=columns)\n",
    "    \n",
    "    return model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params_gridsearch(df_x, df_y, orig_model_with_name, default_params, params_grid,\n",
    "                                columns=None, seed=42, cv=5):\n",
    "    \n",
    "    orig_model_name, orig_model = orig_model_with_name\n",
    "    orig_model_name+='_GS'\n",
    "    i=1\n",
    "    \n",
    "    model_new = GridSearchCV(orig_model(**default_params), params_grid, cv=cv, verbose=1, scoring='roc_auc', n_jobs=4)\n",
    "    \n",
    "    model_with_name = (orig_model_name,model_new)\n",
    "    model, auc = SF.full_framework_wrapper(df_x, df_y, model_with_name,columns=columns)\n",
    "    \n",
    "    return model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "> https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params So Far: {} {}\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:   14.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: decision_tree_GSF - AUC: 0.5729 - AUCPR:0.0745 - Accuracy: 0.9126 \n",
      "Best Params So Far: {} {'criterion': 'entropy'}\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  35 out of  35 | elapsed:   12.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: decision_tree_GSF - AUC: 0.5786 - AUCPR:0.0774 - Accuracy: 0.9126 \n",
      "Best Params So Far: {} {'criterion': 'entropy', 'max_features': 0.7000000000000001}\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KeyboardInterrupt: Proceso muy costoso en tiempo. Se corta la ejecución, que ya sus resultados fueron guardados\n"
     ]
    }
   ],
   "source": [
    "list_of_progressive_params = [{'criterion':['gini','entropy']},\n",
    "                              {'max_features': np.arange(0.1,0.8,0.1)},\n",
    "                              {'max_depth': np.linspace(1, 32, 5, endpoint=True)},\n",
    "                              {'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True)},\n",
    "                              {'min_samples_leaf': np.arange(0.0001,0.5,0.1)}\n",
    "]\n",
    "\n",
    "model_with_name = ('decision_tree', DecisionTreeClassifier)\n",
    "\n",
    "best_params_decision_tree = find_best_params(df_users,df_y,model_with_name, {},list_of_progressive_params, columns=columnas_a_mano) \n",
    "best_params_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_progressive_params = {'criterion':['gini','entropy'],\n",
    "                              'max_features': np.arange(0.1,0.8,0.1),\n",
    "                              'max_depth': np.linspace(1, 32, 5, endpoint=True),\n",
    "                              'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                              'min_samples_leaf': np.arange(0.0001,0.5,0.1)\n",
    "                             }\n",
    "\n",
    "\n",
    "model_with_name = ('decision_tree', DecisionTreeClassifier)\n",
    "\n",
    "best_params_decision_tree = find_best_params_random(df_users,df_y,model_with_name, {}, list_of_progressive_params, n_iter=100, columns=columnas_a_mano) \n",
    "best_params_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_progressive_params = {'criterion':['gini','entropy'],\n",
    "                              'max_features': np.arange(0.1,0.8,0.1),\n",
    "                              'max_depth': np.linspace(1, 32, 5, endpoint=True),\n",
    "                              'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "                              'min_samples_leaf': np.arange(0.0001,0.5,0.1)\n",
    "                             }\n",
    "\n",
    "\n",
    "model_with_name = ('decision_tree', DecisionTreeClassifier)\n",
    "\n",
    "best_params_decision_tree = find_best_params_gridsearch(df_users,df_y,model_with_name, {}, list_of_progressive_params, columns=columnas_a_mano) \n",
    "best_params_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "> https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "   \n",
    "list_of_progressive_params = [{'n_estimators':[1, 2, 4, 8, 16, 32, 64, 100, 200]},\n",
    "                              {'criterion':['gini','entropy']},\n",
    "                              {'max_features': np.arange(0.1,0.4,0.1)},\n",
    "                              {'max_depth': np.linspace(1, 32, 3, endpoint=True)},\n",
    "                              {'min_samples_split': np.arange(0.1, 1.0, 0.1)},\n",
    "                              {'min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True)}\n",
    "                   ]  \n",
    "\n",
    "model_with_name = ('random_forest', RandomForestClassifier)\n",
    "\n",
    "best_params_random_forest = find_best_params(df_users,df_y,model_with_name, {},list_of_progressive_params, columns=columnas_a_mano) \n",
    "best_params_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "> https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb #conda install -c conda-forge xgboost \n",
    "\n",
    "# Pls dejar, dio muy bien y no es muy replicable\n",
    "\"\"\"\n",
    "0.8695\n",
    "\n",
    "{'learning_rate': 0.1,\n",
    " 'objective': 'binary:logistic',\n",
    " 'n_estimators': 16,\n",
    " 'scale_pos_weight': 2,\n",
    " 'max_depth': 4,\n",
    " 'min_child_weight': 5,\n",
    " 'gamma': 0.0,\n",
    " 'colsample_bytree': 0.7500000000000001,\n",
    " 'subsample': 0.7,\n",
    " 'colsample_bylevel': 0.65}\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "list_of_progressive_params = [\n",
    "                              {'objective': ['binary:logistic'],'learning_rate':np.arange(0.1,0.5,0.1)},\n",
    "                              {'n_estimators':np.arange(16,116,15)},\n",
    "                              {'scale_pos_weight':np.arange(2,6,1)},\n",
    "                              {'max_depth':np.arange(4,12,1),'min_child_weight':np.arange(1,10,1)},\n",
    "                              {'gamma':np.arange(0,0.5,0.1)},\n",
    "                              {'subsample':np.arange(0.6,1,0.1),'colsample_bytree':np.arange(0.6,0.91,0.05)},\n",
    "                              {'colsample_bylevel':np.arange(0.6,0.91,0.05)}#,\n",
    "                             # {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]} # Empeoraba muchísimo con esto, y Luis dijo que no importaba\n",
    "                   ]\n",
    "\n",
    "model_with_name = ('xgbost', xgb.XGBClassifier)\n",
    "\n",
    "best_params_xgboost = find_best_params(df_users,df_y,model_with_name,{}, list_of_progressive_params, columns=columnas_a_mano) \n",
    "best_params_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_progressive_params = [\n",
    "                              {'objective': ['binary:logistic','reg:linear'],'learning_rate':np.arange(0.1,0.5,0.1)},\n",
    "                              {'n_estimators':np.arange(16,116,15)},\n",
    "                              {'scale_pos_weight':np.arange(2,6,1)},\n",
    "                              {'max_depth':np.arange(4,12,1),'min_child_weight':np.arange(1,10,1)},\n",
    "                              {'gamma':np.arange(0,0.5,0.1)},\n",
    "                              {'subsample':np.arange(0.6,1,0.1),'colsample_bytree':np.arange(0.6,0.91,0.05)},\n",
    "                              {'colsample_bylevel':np.arange(0.6,0.91,0.05)}#,\n",
    "                             # {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]} # Empeoraba muchísimo con esto, y Luis dijo que no importaba\n",
    "                   ]\n",
    "\n",
    "model_with_name = ('xgbost', xgb.XGBClassifier)\n",
    "\n",
    "best_params_xgboost = find_best_params(df_users_norm,df_y,model_with_name,columnas_a_mano,{}, list_of_progressive_params) \n",
    "best_params_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "list_of_progressive_params = [\n",
    "                              {'n_neighbors': np.arange(1,30)},\n",
    "                              {'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']},\n",
    "                              {'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\n",
    "                   ]\n",
    "\n",
    "model_with_name = ('knn', KNeighborsClassifier)\n",
    "\n",
    "best_params_knn = find_best_params(df_users, df_y, model_with_name, {},list_of_progressive_params, seed=-1, normalize=True) \n",
    "best_params_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM\n",
    "\n",
    "> https://www.kaggle.com/sz8416/simple-bayesian-optimization-for-lightgbm\n",
    "> https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb #conda install -c conda-forge lightgbm \n",
    "\n",
    "\"\"\"\n",
    "0.8688\n",
    "\n",
    "{'objective': 'binary',\n",
    " 'learning_rate': 0.01,\n",
    " 'n_estimators': 190,\n",
    " 'num_leaves': 27,\n",
    " 'feature_fraction': 0.9000000000000001,\n",
    " 'bagging_fraction': 0.8,\n",
    " 'max_depth': 4,\n",
    " 'lambda_l2': 2,\n",
    " 'min_split_gain': 0.01,\n",
    " 'min_child_weight': 10.00001}\n",
    "\"\"\"\n",
    "\n",
    "list_of_progressive_params = [{'objective':['binary']},\n",
    "                             {'learning_rate':[0.005,0.01,0.05,0.1,0.3]},\n",
    "                             {'n_estimators':np.arange(25,200,15)},\n",
    "                             {'num_leaves': np.arange(24, 45,3)},\n",
    "                             {'feature_fraction': np.arange(0.1, 0.91, 0.2)},   \n",
    "                             {'bagging_fraction': np.arange(0.8, 1.01, 0.1)},\n",
    "                             {'max_depth': np.arange(3, 12, 1)},\n",
    "                            #{'lambda_l1': np.arange(0, 5)}, # Restaba mucho\n",
    "                             {'lambda_l2': np.arange(0, 3)},\n",
    "                             {'min_split_gain': [0.001, 0.01, 0.1]},\n",
    "                             {'min_child_weight': [1e-05]+np.arange(5, 11)}\n",
    "                             ]\n",
    "\n",
    "\n",
    "model_with_name = ('lightgbm', lgb.LGBMClassifier)\n",
    "\n",
    "best_params_lightgbm= find_best_params(df_users,df_y,model_with_name,{}, list_of_progressive_params) \n",
    "best_params_lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Random (lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_progressive_params = {'objective':['binary'],\n",
    "                             'learning_rate':[0.005,0.01,0.05,0.1,0.3],\n",
    "                             'n_estimators':np.arange(25,200,15),\n",
    "                             'num_leaves': np.arange(24, 45,3),\n",
    "                             'feature_fraction': np.arange(0.1, 0.91, 0.2),   \n",
    "                             'bagging_fraction': np.arange(0.8, 1.01, 0.1),\n",
    "                             'max_depth': np.arange(3, 12, 1),\n",
    "                             'lambda_l2': np.arange(0, 3),\n",
    "                             'min_split_gain': [0.001, 0.01, 0.1],\n",
    "                             'min_child_weight': [1e-05]+np.arange(5, 11)\n",
    "                             }\n",
    "\n",
    "\n",
    "model_with_name = ('lightgbm', lgb.LGBMClassifier)\n",
    "\n",
    "best_params_decision_tree = find_best_params_random(df_users,df_y,model_with_name, {}, list_of_progressive_params, n_iter=100) \n",
    "best_params_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Former Champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last best parameters \n",
    "# AUC: 0.0700 con todo el dataset\n",
    "# AUC: 0.8711 con columnas a mano\n",
    "params = {'objective': 'binary',\n",
    " 'num_leaves': 36,\n",
    " 'n_estimators': 70,\n",
    " 'min_split_gain': 0.01,\n",
    " 'min_child_weight': 5.00001,\n",
    " 'max_depth': 4,\n",
    " 'learning_rate': 0.05,\n",
    " 'lambda_l2': 0,\n",
    " 'feature_fraction': 0.7000000000000001,\n",
    " 'bagging_fraction': 1.0}\n",
    "\n",
    "model_with_name = ('lightgbm', lgb.LGBMClassifier(**params))\n",
    "SF.full_framework_wrapper(df_users, df_y, model_with_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "list_of_progressive_params = [{'hidden_layer_sizes':[(4,7), (4,4), (4,3,2)]},\n",
    "                              {'activation':['relu', 'logistic']},\n",
    "                              {'alpha':[1e-06,1e-05,1e-04,1e-03,1e-02,1e-01,1]},\n",
    "                              {'beta_1':[0.7,0.91,0.05]},\n",
    "                              {'beta_2':[0.75, 0.86, 0.05]},\n",
    "                              {'early_stopping':[False]},\n",
    "                              {'epsilon':[1e-07,1e-08]},\n",
    "                              {'learning_rate':['constant', 'adaptive']},\n",
    "                              {'solver':['adam', 'lbfgs']},\n",
    "                              {'validation_fraction':np.arange(0.15,0.26,0.05)}\n",
    "                             ]\n",
    "    \n",
    "    \n",
    "model_with_name = ('neuralnetwork', MLPClassifier)\n",
    "\n",
    "\"\"\"\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_users_norm = pd.DataFrame(min_max_scaler.fit_transform(df_users.values))\n",
    "df_users_norm.columns = df_users.columns\n",
    "df_users_norm.index = df_users.index\n",
    "\"\"\"\n",
    "\n",
    "best_params_neuralnetwork = find_best_params(df_users, df_y, model_with_name, {}, list_of_progressive_params, normalize=True)\n",
    "best_params_neuralnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "> https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning-docpage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb #conda install -c conda-forge catboost \n",
    "\n",
    "list_of_progressive_params = [{'random_strength':[42],'eval_metric':['AUC'],'iterations': [80, 100,256,465,678,1000]},\n",
    "                             {'learning_rate':[0.01,0.05,0.1,0.3]},\n",
    "                             {'depth':np.arange(1,12,1)},\n",
    "                             {'l2_leaf_reg':np.arange(2,10,1)},\n",
    "                             ]\n",
    "\n",
    "model_with_name = ('catboost', cb.CatBoostClassifier)\n",
    "\n",
    "best_params_catboost = find_best_params(df_users,df_y,model_with_name,{'verbose':True}, list_of_progressive_params, cv=2,columns=columnas_a_mano)\n",
    "best_params_catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "> https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "> https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC  \n",
    "\n",
    "list_of_progressive_params = [\n",
    "                             {'max_leaf_nodes': [None]},\n",
    "                             {'min_weight_fraction_leaf': [0]},\n",
    "                             {'learning_rate': [0.1]},\n",
    "                             {'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True)},\n",
    "                             {'min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True)},  \n",
    "                             {'max_features' : list(range(1,len(columnas_a_mano)))},\n",
    "                             {'max_depth': np.linspace(1, 32, 32, endpoint=True)},\n",
    "                             {'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 20]},\n",
    "                             {'subsample': np.arange(0.8, 1)},\n",
    "                             {'loss': ['deviance']},\n",
    "                             {'warm_start': [False]},\n",
    "                             {'presort': ['auto']}\n",
    "                             ]\n",
    "    \n",
    "model_with_name = ('gradient_boosting', GBC)\n",
    "\n",
    "best_params_boosting= find_best_params(df_users,df_y,model_with_name, list_of_progressive_params, columns=columnas_a_mano) \n",
    "best_params_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Entrega: Como persistir modelos\n",
    "\n",
    "GridSearchCV y RandomizedSearchCV no solo encuentran los mejores hiper-parametros, también hacen Cross-Validation sobre los modelos, y por eso es importante quedarse con el modelo una vez que termino la ejecución, sin necesariamente correrlo de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lightgbm_RS - AUC: 0.8595 - AUCPR:0.2359 - Accuracy: 0.9494 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "          fit_params=None, iid='warn', n_iter=15, n_jobs=4,\n",
       "          param_distributions={'n_estimators': array([ 25,  40,  55,  70,  85, 100, 115, 130, 145, 160, 175, 190]), 'max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import lightgbm as lgb #conda install -c conda-forge lightgbm \n",
    "\n",
    "def find_best_params_random(df_x, df_y, orig_model_with_name, default_params, params_grid,\n",
    "                            n_iter=15, columns=None, seed=0, cv=5):\n",
    "    \n",
    "    orig_model_name, orig_model = orig_model_with_name\n",
    "    orig_model_name+='_RS'\n",
    "    i=1\n",
    "\n",
    "    model_new = RandomizedSearchCV(orig_model(**default_params,random_state=seed), params_grid, n_iter=n_iter, cv=cv, verbose=1, scoring='roc_auc', n_jobs=4)\n",
    "\n",
    "    model_with_name = (orig_model_name,model_new)\n",
    "    model, auc = SF.full_framework_wrapper(df_x, df_y, model_with_name,columns=columns)\n",
    "    \n",
    "    return model\n",
    "\n",
    "list_of_progressive_params = {'n_estimators':np.arange(25,200,15),\n",
    "                              'max_depth': np.arange(3, 12, 1)}\n",
    "\n",
    "\n",
    "model_with_name = ('lightgbm', lgb.LGBMClassifier)\n",
    "\n",
    "best_model_lightgbm = find_best_params_random(df_users,df_y,model_with_name,{}, list_of_progressive_params) \n",
    "best_model_lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardarlo:\n",
    "model_file = open('lightgbm-rg-2.0-8718.pickle.dat','wb')\n",
    "pickle.dump(best_model_lightgbm, model_file)\n",
    "model_file.close()\n",
    "\n",
    "# Cargarlo:\n",
    "model_file = open('lightgbm-rg-2.0-8718.pickle.dat','rb')\n",
    "loaded_model = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=0, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "          fit_params=None, iid='warn', n_iter=15, n_jobs=4,\n",
       "          param_distributions={'n_estimators': array([ 25,  40,  55,  70,  85, 100, 115, 130, 145, 160, 175, 190]), 'max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contenido",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
