{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "\n",
    "# Submission Framework\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se definen unas simples funciones de I/O para armar las postulaciones de predicciones del trabajo práctico.\n",
    "\n",
    "Uso:\n",
    "\n",
    "1. Crea la matriz `X` y el vector `y` para entrenar\n",
    "\n",
    "2. Split para generar los set de entrenamiento y de prueba\n",
    "\n",
    "3. Se ejecuta el algoritmo de ML (debe devolver un dataframe con person en el indice y labels como unica columna)\n",
    "\n",
    "4. Se ve la precisión de la predicción\n",
    "\n",
    "5. Se obtiene la métrica AUC (Area Under the Receiving Operating Characteristic Curve)\n",
    "\n",
    "6. Se predicen las probabilidades\n",
    "\n",
    "7. Se ve información relevante de la ejecución\n",
    "\n",
    "8. Se guarda como un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False).set_index('person')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv').groupby('person').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_equals(x,y):\n",
    "    if not (x==y): \n",
    "        msg = f'{x} no equivale a {y}'\n",
    "        print(msg)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def df_label_xor(df1, df2):\n",
    "    merged = df1.merge(df2, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    merged = merged.query('_merge != \"both\"')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr1_extract_X_y(df, df_y):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df.merge(df_y, how='inner', left_index= True, right_index=True)\n",
    "    if not assert_equals(len(data), 19414): return\n",
    "    \n",
    "    X = data.drop('label', axis=1).values\n",
    "        \n",
    "    y = df_y.values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fr2_train_test_split(X, y, seed, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def fr4_accuracy_score(X_test, y_test, model, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if not assert_equals(len(y_test), len(y_pred)): return\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())\n",
    "    return accuracy\n",
    "\n",
    "def fr5_auc(X_test, y_test, model, model_name):\n",
    "    auc = make_scorer(roc_auc_score, needs_threshold=True)(model, X_test, y_test)\n",
    "    return auc\n",
    "\n",
    "def fr6_extract_X_to_predict(df, df_y, model):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df_label_xor(df, df_y)\n",
    "    data = data.drop(['label', '_merge'], axis=1)\n",
    "\n",
    "    if not assert_equals(len(data), 19415): return\n",
    "\n",
    "    predictions = model.predict_proba(data.values)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for i in predictions:\n",
    "        predictions_list.append(i[1])\n",
    "    predictions_final = np.array(predictions_list)\n",
    "        \n",
    "    return data, predictions_final\n",
    "\n",
    "\n",
    "def fr7_print_information(df, model, X_to_predict, with_features_importance):\n",
    "    if not with_features_importance:\n",
    "        return None\n",
    "    \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                  index=df.columns,\n",
    "                                  columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    return feature_importances\n",
    "    \n",
    "    \n",
    "def fr8_to_csv(df, predictions, model_name, accuracy):\n",
    "    submission = df\n",
    "    submission['label'] = predictions\n",
    "    submission = submission['label']\n",
    "    \n",
    "    if not assert_equals(len(submission), 19415): return\n",
    "    \n",
    "    name_csv = f'submission-{model_name}-{accuracy:.4f}.csv'\n",
    "    submission.to_csv(name_csv, header=True)\n",
    "    return name_csv, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_framework_wrapper(model_name, model_function, \n",
    "                           columns=df_users.columns.tolist(), test_size=0.34, seed=42, \n",
    "                           verbosity=0, all_in=False, submit=False,feature_importance_print=False):\n",
    "    \n",
    "    model_df_x = df_users[columns]\n",
    "    model_df_y = df_y\n",
    "    \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "    if all_in:\n",
    "        model = model_function(X, y, seed)\n",
    "    else:\n",
    "        model = model_function(X_train, y_train, seed)\n",
    "    accuracy = fr4_accuracy_score(X_test, y_test, model, model_name)\n",
    "    auc = fr5_auc(X_test, y_test, model, model_name)\n",
    "    X_to_predict, predictions = fr6_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "       \n",
    "    if verbosity>=1: print('Model: {} - Columns: {} (AUC: {:.4f})'.format(model_name, columns, auc))  \n",
    "    elif verbosity >=0: print('Model: {} - Accuracy: {:.4f} - AUC: {:.4f}'.format(model_name, accuracy, auc))  \n",
    "    \n",
    "    if feature_importance_print: \n",
    "        feature_importances = fr7_print_information(model_df_x, model, X_to_predict, True)\n",
    "        display(feature_importances)\n",
    "            \n",
    "    if submit:\n",
    "        csv_name, submission = fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "        display(csv_name)\n",
    "        return model, auc, csv_name\n",
    "    \n",
    "    return model, auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
