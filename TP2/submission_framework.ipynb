{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "\n",
    "# Submission Framework\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se definen unas simples funciones de I/O para armar las postulaciones de predicciones del trabajo práctico.\n",
    "\n",
    "Uso:\n",
    "\n",
    "1. Crea la matriz `X` y el vector `y` para entrenar\n",
    "\n",
    "2. Split para generar los set de entrenamiento y de prueba\n",
    "\n",
    "3. Se ejecuta el algoritmo de ML (debe devolver un dataframe con person en el indice y labels como unica columna)\n",
    "\n",
    "4. Se ve la precisión de la predicción\n",
    "\n",
    "5. Se obtiene la métrica AUC (Area Under the Receiving Operating Characteristic Curve)\n",
    "\n",
    "6. Se predicen las probabilidades\n",
    "\n",
    "7. Se ve información relevante de la ejecución\n",
    "\n",
    "8. Se guarda como un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_equals(x,y):\n",
    "    if not (x==y): \n",
    "        msg = f'{x} no equivale a {y}'\n",
    "        print(msg)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def df_label_xor(df1, df2):\n",
    "    merged = df1.merge(df2, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    merged = merged.query('_merge != \"both\"')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr1_extract_X_y(df, df_y):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df.merge(df_y, how='inner', left_index= True, right_index=True)\n",
    "    if not assert_equals(len(data), 19414): return\n",
    "    \n",
    "    X = data.drop('label', axis=1).values\n",
    "    y = df_y.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fr2_train_test_split(X, y, seed, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def fr4_accuracy_score(X_test, y_test, model, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_test)\n",
    "    print(y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())\n",
    "    print('{} accuracy score: {}'.format(model_name, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def f45_auc():\n",
    "    return\n",
    "\n",
    "def fr6_extract_X_to_predict(df, df_y, model):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df_label_xor(df, df_y)\n",
    "    data = data.drop(['label', '_merge'], axis=1)\n",
    "\n",
    "    if not assert_equals(len(data), 19415): return\n",
    "\n",
    "    predictions = model.predict_proba(data.values)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for i in predictions:\n",
    "        predictions_list.append(i[1])\n",
    "    predictions_final = np.array(predictions_list)\n",
    "    \n",
    "    print(\"[fr5] Predictions: {}\".format(predictions_final))\n",
    "\n",
    "    return data, predictions_final\n",
    "\n",
    "\n",
    "def fr7_print_information(df, model, X_to_predict, with_features_importance):\n",
    "    print('[fr6] Ammount of 1s: {}'.format(model.predict(X_to_predict).sum()))\n",
    "    \n",
    "    if not with_features_importance:\n",
    "        return None\n",
    "    \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                  index=df.columns,\n",
    "                                  columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    return feature_importances\n",
    "    \n",
    "    \n",
    "def fr8_to_csv(df, predictions, model_name, accuracy):\n",
    "    \n",
    "    submission = df\n",
    "    submission['label'] = predictions\n",
    "    submission = submission['label']\n",
    "    \n",
    "    if not assert_equals(len(submission), 19415): return\n",
    "    \n",
    "    name_csv = f'submission-{model_name}-{accuracy:.2f}.csv'\n",
    "    submission.to_csv(name_csv, header=True)\n",
    "    return name_csv, submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
