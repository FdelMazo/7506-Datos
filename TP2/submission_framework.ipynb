{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "\n",
    "# Submission Framework\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se definen unas simples funciones de I/O para armar las postulaciones de predicciones del trabajo práctico.\n",
    "\n",
    "Uso:\n",
    "\n",
    "1. Crea la matriz `X` y el vector `y` para entrenar\n",
    "\n",
    "2. Split para generar los set de entrenamiento y de prueba\n",
    "\n",
    "3. Se ejecuta el algoritmo de ML (debe devolver un dataframe con person en el indice y labels como unica columna)\n",
    "\n",
    "4. Se ve la precisión de la predicción\n",
    "\n",
    "5. Se obtiene la métrica AUC (Area Under the Receiving Operating Characteristic Curve)\n",
    "\n",
    "6. Se predicen las probabilidades\n",
    "\n",
    "7. Se ve información relevante de la ejecución\n",
    "\n",
    "8. Se guarda como un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "df_users = pd.read_csv('data/user-features.csv',low_memory=False).set_index('person')\n",
    "df_y = pd.read_csv('data/labels_training_set.csv').groupby('person').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_equals(x,y):\n",
    "    if not (x==y): \n",
    "        msg = f'{x} no equivale a {y}'\n",
    "        print(msg)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def df_label_xor(df1, df2):\n",
    "    merged = df1.merge(df2, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    merged = merged.query('_merge != \"both\"')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr1_extract_X_y(df, df_y, normalize=False):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df.merge(df_y, how='inner', left_index= True, right_index=True)\n",
    "    if not assert_equals(len(data), 19414): return\n",
    "    \n",
    "    X = data.drop('label', axis=1).values\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X_scaled = min_max_scaler.fit_transform(X)\n",
    "        X = X_scaled\n",
    "        \n",
    "    y = df_y.values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fr2_train_test_split(X, y, seed, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def fr4_metric_score(X_test, y_test, model, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if not assert_equals(len(y_test), len(y_pred)): return\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())    \n",
    "    auc = make_scorer(roc_auc_score, needs_threshold=True)(model, X_test, y_test)\n",
    "\n",
    "    return accuracy, auc\n",
    "\n",
    "\n",
    "def fr5_extract_X_to_predict(df, df_y, model):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df_label_xor(df, df_y)\n",
    "    data = data.drop(['label', '_merge'], axis=1)\n",
    "\n",
    "    if not assert_equals(len(data), 19415): return\n",
    "\n",
    "    predictions = model.predict_proba(data.values)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for i in predictions:\n",
    "        predictions_list.append(i[1])\n",
    "    predictions_final = np.array(predictions_list)\n",
    "        \n",
    "    return data, predictions_final\n",
    "\n",
    "\n",
    "def fr6_print_information(df, model, X_to_predict, with_features_importance):\n",
    "    if not with_features_importance:\n",
    "        return None\n",
    "    \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                  index=df.columns,\n",
    "                                  columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    return feature_importances\n",
    "    \n",
    "    \n",
    "def fr7_train_final_model(algorithm, X, y):\n",
    "    return algorithm.fit(X, y)\n",
    "    \n",
    "    \n",
    "def fr8_to_csv(df, predictions, model_name, accuracy):\n",
    "    submission = df\n",
    "    submission['label'] = predictions\n",
    "    submission = submission['label']\n",
    "    \n",
    "    if not assert_equals(len(submission), 19415): return\n",
    "    \n",
    "    name_csv = f'submission-{model_name}-{accuracy:.4f}.csv'\n",
    "    submission.to_csv(name_csv, header=True)\n",
    "    return name_csv, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_framework_wrapper(model_name, model_function, \n",
    "                           columns=df_users.columns.tolist(), normalize=False, \n",
    "                           test_size=0.34, seed=42, \n",
    "                           verbosity=0, all_in=False, submit=False,n_ensamble=0):\n",
    "    \n",
    "    model_df_x = df_users[columns]\n",
    "    model_df_y = df_y\n",
    "    \n",
    "    if n_ensamble: model_name+=f'_ensamble_{n_ensamble}'\n",
    "    if all_in: model_name+='_all_in'\n",
    "    \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y, normalize)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "\n",
    "    if not n_ensamble == 0:\n",
    "        \n",
    "        total_predictions = 0\n",
    "        tmp_seed = seed\n",
    "        accuracy, auc = 0,0\n",
    "        for i in range(n_ensamble):\n",
    "            print(f'Iteración {i+1} de ensamble de {n_ensamble}')\n",
    "            tmp_seed = tmp_seed + i\n",
    "            model = model_function(X_train, y_train, tmp_seed)\n",
    "            accuracy_tmp, auc_tmp = fr4_metric_score(X_test, y_test, model, model_name)\n",
    "            accuracy += accuracy_tmp\n",
    "            auc += auc_tmp\n",
    "            if all_in: model = model_function(X, y, tmp_seed)            \n",
    "            X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "            total_predictions = total_predictions + predictions\n",
    "\n",
    "        predictions = total_predictions / n_ensamble\n",
    "        accuracy /= n_ensamble\n",
    "        auc /= n_ensamble\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] < 0:\n",
    "                print(\"WARNING: prediction[{}] = {}\".format(i, predictions[i]))\n",
    "                predictions[i] = 0   \n",
    "        \n",
    "        if submit:\n",
    "            print('asd')\n",
    "            csv_name, submission = fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "            display(csv_name)\n",
    "            message = f\"{model_name} - {model.get_params()} - {columns}\"\n",
    "            display(message)\n",
    "            return model, auc, csv_name,message\n",
    "        \n",
    "        return model, auc\n",
    "    \n",
    "    model = model_function(X_train, y_train, seed)\n",
    "    accuracy, auc = fr4_metric_score(X_test, y_test, model, model_name)\n",
    "    \n",
    "    if all_in:\n",
    "        model = model_function(X, y, seed)\n",
    "    \n",
    "    X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "       \n",
    "    if verbosity>=1: print('Model: {} - Columns: {} (AUC: {:.4f})'.format(model_name, columns, auc))  \n",
    "    elif verbosity >=0: print('Model: {} - Accuracy: {:.4f} - AUC: {:.4f}'.format(model_name, accuracy, auc))  \n",
    "    \n",
    "    if submit:\n",
    "        csv_name, submission = fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "        display(csv_name)\n",
    "        message = f\"{model_name} - {model.get_params()} - {columns}\"\n",
    "        display(message)\n",
    "        return model, auc, csv_name, message\n",
    "    \n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model_name, model_function, \n",
    "                            columns=df_users.columns.tolist(), normalize=False,\n",
    "                            test_size=0.34, seed=42):\n",
    "    \n",
    "    model_df_x = df_users[columns]\n",
    "    model_df_y = df_y\n",
    "    \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y, normalize)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "    model = model_function(X_train, y_train, seed)\n",
    "    accuracy, auc = fr4_metric_score(X_test, y_test, model, model_name)\n",
    "    X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "       \n",
    "    return fr6_print_information(model_df_x, model, X_to_predict, True)\n",
    "\n",
    "def get_full_features():\n",
    "    return df_users.columns.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
