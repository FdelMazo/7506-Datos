{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [75.06 / 95.58] Organización de Datos <br> Trabajo Práctico 2: Machine Learning\n",
    "\n",
    "# Submission Framework\n",
    "\n",
    "**Grupo 30: Datatouille**\n",
    "\n",
    "**http://fdelmazo.github.io/7506-Datos/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se definen unas simples funciones de I/O para armar las postulaciones de predicciones del trabajo práctico.\n",
    "\n",
    "Uso:\n",
    "\n",
    "1. Crea la matriz `X` y el vector `y` para entrenar\n",
    "\n",
    "2. Split para generar los set de entrenamiento y de prueba\n",
    "\n",
    "3. Se ejecuta el algoritmo de ML (debe devolver un dataframe con person en el indice y labels como unica columna)\n",
    "\n",
    "4. Se obtienen las distintas métricas de la predicción para obtener la precisión\n",
    "\n",
    "6. Se predicen las probabilidades\n",
    "\n",
    "7. Se ve información relevante de la ejecución\n",
    "\n",
    "8. Se guarda como un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_equals(x,y):\n",
    "    if not (x==y): \n",
    "        msg = f'{x} no equivale a {y}'\n",
    "        print(msg)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def df_label_xor(df1, df2):\n",
    "    merged = df1.merge(df2, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    merged = merged.query('_merge != \"both\"')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr1_extract_X_y(df, df_y, normalize=False):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df.merge(df_y, how='inner', left_index= True, right_index=True)\n",
    "    if not assert_equals(len(data), 19414): return\n",
    "    \n",
    "    X = data.drop('label', axis=1).values\n",
    "    \n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        X_scaled = min_max_scaler.fit_transform(X)\n",
    "        X = X_scaled\n",
    "        \n",
    "    y = df_y.values\n",
    "    y.shape = y.shape[0]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fr2_train_test_split(X, y, seed, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=seed)\n",
    "    \n",
    "    y_train.shape = y_train.shape[0]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def fr4_metric_score(X_test, y_test, model, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if not assert_equals(len(y_test), len(y_pred)): return\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())    \n",
    "    auc = make_scorer(roc_auc_score, needs_threshold=True)(model, X_test, y_test)\n",
    "    aucpr = make_scorer(average_precision_score, needs_threshold=True)(model, X_test, y_test)\n",
    "\n",
    "    return accuracy, auc, aucpr\n",
    "\n",
    "\n",
    "def fr5_extract_X_to_predict(df, df_y, model, tipo='predict_proba'):\n",
    "    if not assert_equals(len(df), 38829): return\n",
    "    if not assert_equals(len(df_y), 19414): return\n",
    "    \n",
    "    data = df_label_xor(df, df_y)\n",
    "    data = data.drop(['label', '_merge'], axis=1)\n",
    "\n",
    "    if not assert_equals(len(data), 19415): return\n",
    "\n",
    "    if tipo == 'predict_proba': predictions = model.predict_proba(data.values)\n",
    "    else: predictions = model.predict(data.values)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for i in predictions:\n",
    "        predictions_list.append(i[1])\n",
    "    predictions_final = np.array(predictions_list)\n",
    "        \n",
    "    return data, predictions_final\n",
    "\n",
    "\n",
    "def fr6_print_information(df, model, X_to_predict, with_features_importance):\n",
    "    if not with_features_importance:\n",
    "        return None\n",
    "    \n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                  index=df.columns,\n",
    "                                  columns=['importance'])\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    return feature_importances\n",
    "    \n",
    "    \n",
    "def fr7_train_final_model(algorithm, X, y):\n",
    "    return algorithm.fit(X, y)\n",
    "    \n",
    "    \n",
    "def fr8_to_csv(df, predictions, name, auc):\n",
    "    name = name.replace('+','-')\n",
    "    submission = df\n",
    "    submission['label'] = predictions\n",
    "    submission = submission['label']\n",
    "    \n",
    "    if not assert_equals(len(submission), 19415): return\n",
    "    \n",
    "    name_csv = f'submission-{name}-{auc:.4f}.csv'\n",
    "    submission.to_csv(name_csv, header=True)\n",
    "    return name_csv, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(df_x, df_y, model_with_name, \n",
    "                           columns=None,\n",
    "                           normalize=False, \n",
    "                           test_size=0.34, seed=42):\n",
    "        \n",
    "    if not columns: columns=df_x.columns.tolist()\n",
    "    model_df_x = df_x[columns]   \n",
    "    model_df_y = df_y\n",
    "\n",
    "    model_name,model = model_with_name   \n",
    "    \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y, normalize)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "        \n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "                \n",
    "    return fr6_print_information(model_df_x, model, X_to_predict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_framework_normal(df_x, df_y, \n",
    "                           model_with_name, \n",
    "                           columns=None,\n",
    "                           normalize=False, \n",
    "                           test_size=0.34, seed=42, \n",
    "                           verbosity=0, \n",
    "                           all_in=False,\n",
    "                           n_ensamble=0,\n",
    "                           submit=False):\n",
    "        \n",
    "    if not columns: columns=df_x.columns.tolist()\n",
    "    model_df_x = df_x[columns]   \n",
    "    model_df_y = df_y\n",
    "\n",
    "    \n",
    "    model_name,model = model_with_name   \n",
    "    if n_ensamble: model_name+=f'_ensamble_{n_ensamble}'\n",
    "    if normalize: model_name+='_normalized'\n",
    "    if all_in: model_name+='_all_in'\n",
    "    \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y, normalize)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "        \n",
    "    model.fit(X_train,y_train)\n",
    "    accuracy, auc, aucpr = fr4_metric_score(X_test, y_test, model, model_name)\n",
    "\n",
    "    if all_in:\n",
    "        model.fit(X,y)\n",
    "\n",
    "    X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "    \n",
    "    if not n_ensamble == 0:\n",
    "        total_predictions = 0\n",
    "        tmp_seed = seed\n",
    "        accuracy, auc, aucpr = 0,0,0\n",
    "        for i in range(n_ensamble):\n",
    "            print(f'Iteración {i+1} de ensamble de {n_ensamble}')\n",
    "            tmp_seed = tmp_seed + i\n",
    "            \n",
    "            model.fit(X_train,y_train)\n",
    "            accuracy_tmp, auc_tmp, aucpr_tmp = fr4_metric_score(X_test, y_test, model, model_name)\n",
    "            accuracy += accuracy_tmp\n",
    "            auc += auc_tmp\n",
    "            aucpr += aucpr_tmp\n",
    "              \n",
    "            if all_in: model = model.fit(X,y)\n",
    "            X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, model)\n",
    "            total_predictions += predictions\n",
    "\n",
    "        predictions = total_predictions / n_ensamble\n",
    "        accuracy /= n_ensamble\n",
    "        auc /= n_ensamble    \n",
    "        aucpr /= n_ensamble    \n",
    "                \n",
    "    if verbosity >=0: \n",
    "        print(f'Model: {model_name} - AUC: {auc:.4f} - AUCPR:{aucpr:.4f} - Accuracy: {accuracy:.4f} ')\n",
    "    if verbosity >=1:\n",
    "        print(f'{columns}')\n",
    "        \n",
    "    if submit:\n",
    "        csv_name, submission = fr8_to_csv(X_to_predict, predictions, model_name, auc)\n",
    "        message = f\"{model_name} - {model.get_params()} - {columns}\"\n",
    "        display(csv_name)\n",
    "        display(message)\n",
    "        return model, auc, csv_name, message\n",
    "    \n",
    "    return model, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_framework_ensemble(df_x, df_y, models_with_name, \n",
    "                           columns=None,\n",
    "                           normalize=False, \n",
    "                           test_size=0.34, seed=42, \n",
    "                           verbosity=0, \n",
    "                           all_in=False,\n",
    "                           n_ensamble=0,\n",
    "                           submit=False,\n",
    "                           tipo='soft'):\n",
    "\n",
    "    if not columns: columns=df_x.columns.tolist()\n",
    "    model_df_x = df_x[columns]   \n",
    "    model_df_y = df_y\n",
    "\n",
    "    models_name,models = models_with_name   \n",
    "    if n_ensamble: models_name+=f'_ensamble_{n_ensamble}'\n",
    "    if normalize: models_name+='_normalized'\n",
    "    if all_in: models_name+='_all_in'\n",
    "            \n",
    "    X, y = fr1_extract_X_y(model_df_x, model_df_y, normalize)\n",
    "    X_train, X_test, y_train, y_test = fr2_train_test_split(X, y, seed, test_size)\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators=models, voting=tipo)          \n",
    "    ensemble = ensemble.fit(X_train, y_train)\n",
    "    auc = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    accuracy = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    auc = max(auc)\n",
    "    accuracy = max(accuracy)\n",
    "    \n",
    "    if all_in:\n",
    "        ensemble.fit(X,y)\n",
    "\n",
    "    if tipo == 'soft': X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, ensemble)\n",
    "    else: X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, ensemble,tipo='predict')\n",
    "    \n",
    "    if not n_ensamble == 0:\n",
    "        total_predictions = 0\n",
    "        tmp_seed = seed\n",
    "        max_seed = seed\n",
    "        accuracy, auc = 0,0\n",
    "        for i in range(n_ensamble):\n",
    "            print(f'Iteración {i+1} de ensamble de {n_ensamble}')\n",
    "            tmp_seed = tmp_seed + i\n",
    "            \n",
    "            ensemble.fit(X_train,y_train)\n",
    "            auc_tmp = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "            accuracy_tmp = cross_val_score(ensemble, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            if np.mean(auc_tmp) > np.mean(auc): \n",
    "                auc = auc_tmp\n",
    "                accuracy = accuracy_tmp\n",
    "                max_seed = tmp_seed\n",
    "            if all_in: ensemble = ensemble.fit(X,y)\n",
    "            auc = np.mean(auc)\n",
    "            accuracy = np.mean(accuracy)\n",
    "            X_to_predict, predictions = fr5_extract_X_to_predict(model_df_x, model_df_y, ensemble)\n",
    "\n",
    "        print(f'El mejor seed: {max_seed}')\n",
    "                \n",
    "    print(f'Model: {models_name} - AUC: {auc:.4f} - Accuracy: {accuracy:.4f}')\n",
    "    if verbosity >=1:\n",
    "        print(f'{columns}')\n",
    "        \n",
    "    if submit:\n",
    "        csv_name, submission = fr8_to_csv(X_to_predict, predictions, models_name, auc)\n",
    "        message = f\"{models_name} - {ensemble.get_params()} - {columns}\"\n",
    "        display(csv_name)\n",
    "        display(message)\n",
    "        return ensemble, auc, csv_name, message\n",
    "    \n",
    "    return ensemble, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_framework_wrapper(df_x, df_y, \n",
    "                           model_with_name, \n",
    "                           columns=None,\n",
    "                           normalize=False, \n",
    "                           test_size=0.34, seed=42, \n",
    "                           verbosity=0, \n",
    "                           all_in=False,\n",
    "                           n_ensamble=0,\n",
    "                           submit=False):\n",
    "    \n",
    "    model_name,model = model_with_name   \n",
    "    if '+' in model_name: \n",
    "        if 'soft' in model_name: return full_framework_ensemble(df_x, df_y,model_with_name,columns,normalize,test_size,seed,verbosity,all_in, n_ensamble,submit,tipo='soft')\n",
    "        else: return full_framework_ensemble(df_x, df_y,model_with_name,columns,normalize,test_size,seed,verbosity,all_in, n_ensamble,submit,tipo='hard')\n",
    "    \n",
    "    else: return full_framework_normal(df_x, df_y,model_with_name,columns,normalize,test_size,seed,verbosity,all_in, n_ensamble,submit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
